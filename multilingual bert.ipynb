{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries \n",
    "This example is carried out in pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed text data \n",
    "train_data = np.load('./storage/fintech_nlp/train_text_morphed.npy', allow_pickle = True) \n",
    "test_data = np.load('./storage/fintech_nlp/test_text_morphed.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745,), (142565,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./storage/fintech_nlp/lgbm_train_df.csv') \n",
    "y_train = train_df['info'] \n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118745,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize using the BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case = False) \n",
    "#train_tokenized = [tokenizer.tokenize(s) for s in train_bert]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad tokenized sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,   164,   100,   166,   100,   100, 69015,  9547,   100,\n",
       "         164,  9638,   100,   100,   166, 10208,   131, 10842, 26565,\n",
       "         100,   100, 69015,  9547,   100,   102,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 128 \n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in train_data] \n",
    "input_ids = pad_sequences(input_ids, maxlen = MAX_LEN, dtype = 'long', truncating = 'post', padding = 'post') \n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Attention Mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "attention_masks = [] \n",
    "for seq in input_ids: \n",
    "    seq_mask = [float(i > 0) for i in seq] \n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "print(attention_masks[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, y_train,\n",
    "                                                                                    random_state = 42, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                                       random_state = 42, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to pytorch tensor \n",
    "train_inputs = torch.tensor(train_inputs) \n",
    "train_labels = torch.tensor(train_labels) \n",
    "train_masks = torch.tensor(train_masks) \n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs) \n",
    "validation_labels = torch.tensor(validation_labels) \n",
    "validation_masks = torch.tensor(validation_masks)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels) \n",
    "train_sampler = RandomSampler(train_data) \n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels) \n",
    "validation_sampler = SequentialSampler(validation_data) \n",
    "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the preprocessing steps for test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in test_data] \n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen = MAX_LEN, dtype = 'long',  truncating = 'post', padding = 'post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_masks = [] \n",
    "for seq in test_input_ids: \n",
    "    seq_mask = [float(i > 0) for i in seq] \n",
    "    test_attention_masks.append(seq_mask)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([142565, 128]), torch.Size([142565, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs.shape, test_masks.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_masks) \n",
    "test_sampler = SequentialSampler(test_data) \n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size, shuffle = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)   \n",
    "epochs = 30\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels): \n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten() \n",
    "    labels_flat = labels.flatten()  \n",
    "    return np.sum(pred_flat == labels_flat)/len(labels_flat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed)) \n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc(inputs, targets):\n",
    "    return F.binary_cross_entropy(inputs,targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42 \n",
    "random.seed(seed_val) \n",
    "np.random.seed(seed_val) \n",
    "torch.manual_seed(seed_val) \n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad() # model gradient initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './storage/bert_multilingual_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:59.\n",
      " Batch   400 of   835. Elapsed: 0:09:19.\n",
      " Batch   500 of   835. Elapsed: 0:11:39.\n",
      " Batch   600 of   835. Elapsed: 0:13:59.\n",
      " Batch   700 of   835. Elapsed: 0:16:19.\n",
      " Batch   800 of   835. Elapsed: 0:18:39.\n",
      "\n",
      "  Average training loss: 0.0746698816\n",
      "  Training epcoh took: 0:19:27\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.98768600\n",
      " average val loss = 0.0580594353\n",
      "  Validation took: 0:00:44\n",
      "saving model\n",
      "\n",
      "======== Epoch 2 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:59.\n",
      " Batch   400 of   835. Elapsed: 0:09:19.\n",
      " Batch   500 of   835. Elapsed: 0:11:39.\n",
      " Batch   600 of   835. Elapsed: 0:13:59.\n",
      " Batch   700 of   835. Elapsed: 0:16:19.\n",
      " Batch   800 of   835. Elapsed: 0:18:38.\n",
      "\n",
      "  Average training loss: 0.0318244666\n",
      "  Training epcoh took: 0:19:27\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.98871867\n",
      " average val loss = 0.0387097001\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 3 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:59.\n",
      " Batch   400 of   835. Elapsed: 0:09:19.\n",
      " Batch   500 of   835. Elapsed: 0:11:39.\n",
      " Batch   600 of   835. Elapsed: 0:13:59.\n",
      " Batch   700 of   835. Elapsed: 0:16:18.\n",
      " Batch   800 of   835. Elapsed: 0:18:38.\n",
      "\n",
      "  Average training loss: 0.0208726830\n",
      "  Training epcoh took: 0:19:27\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99157485\n",
      " average val loss = 0.0347117484\n",
      "  Validation took: 0:00:44\n",
      "saving model\n",
      "\n",
      "======== Epoch 4 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:59.\n",
      " Batch   400 of   835. Elapsed: 0:09:19.\n",
      " Batch   500 of   835. Elapsed: 0:11:38.\n",
      " Batch   600 of   835. Elapsed: 0:13:58.\n",
      " Batch   700 of   835. Elapsed: 0:16:18.\n",
      " Batch   800 of   835. Elapsed: 0:18:38.\n",
      "\n",
      "  Average training loss: 0.0145252528\n",
      "  Training epcoh took: 0:19:27\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99249891\n",
      " average val loss = 0.0315996148\n",
      "  Validation took: 0:00:44\n",
      "saving model\n",
      "\n",
      "======== Epoch 5 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:18.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:17.\n",
      " Batch   800 of   835. Elapsed: 0:18:37.\n",
      "\n",
      "  Average training loss: 0.0109645778\n",
      "  Training epcoh took: 0:19:25\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99207889\n",
      " average val loss = 0.0269966312\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 6 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:18.\n",
      " Batch   500 of   835. Elapsed: 0:11:38.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:17.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0085947082\n",
      "  Training epcoh took: 0:19:25\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99107082\n",
      " average val loss = 0.0314522721\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 7 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:18.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0058047062\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99233090\n",
      " average val loss = 0.0278117117\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 8 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0054460786\n",
      "  Training epcoh took: 0:19:25\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99224690\n",
      " average val loss = 0.0290255956\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 9 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:18.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:17.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0052814965\n",
      "  Training epcoh took: 0:19:25\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99291894\n",
      " average val loss = 0.0251989253\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 10 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:18.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0044483509\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99291894\n",
      " average val loss = 0.0269436929\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 11 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:39.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:18.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0034785580\n",
      "  Training epcoh took: 0:19:25\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99359098\n",
      " average val loss = 0.0272444729\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 12 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0028001002\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99275093\n",
      " average val loss = 0.0277440418\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 13 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0029575199\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99149085\n",
      " average val loss = 0.0325805284\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 14 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:36.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0025343185\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99367499\n",
      " average val loss = 0.0233362932\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 15 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0019559918\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99308695\n",
      " average val loss = 0.0304064546\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 16 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:36.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0021759929\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99300295\n",
      " average val loss = 0.0266206544\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 17 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0015375237\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99241491\n",
      " average val loss = 0.0355805121\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 18 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0015331137\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99342297\n",
      " average val loss = 0.0265588965\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 19 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:36.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0013838813\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99392701\n",
      " average val loss = 0.0255683493\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 20 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:36.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:34.\n",
      "\n",
      "  Average training loss: 0.0007329332\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99333897\n",
      " average val loss = 0.0308133606\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 21 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0008359636\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99300295\n",
      " average val loss = 0.0305560604\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 22 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:57.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0009375192\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99342297\n",
      " average val loss = 0.0301686283\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 23 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0006587546\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99325496\n",
      " average val loss = 0.0326865055\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 24 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0005705079\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99300295\n",
      " average val loss = 0.0340202376\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 25 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:36.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:34.\n",
      "\n",
      "  Average training loss: 0.0004553418\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99317096\n",
      " average val loss = 0.0333363190\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 26 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:57.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:36.\n",
      "\n",
      "  Average training loss: 0.0005470848\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99317096\n",
      " average val loss = 0.0324869826\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 27 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:16.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0002468217\n",
      "  Training epcoh took: 0:19:24\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99317096\n",
      " average val loss = 0.0349557772\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 28 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n",
      " Batch   400 of   835. Elapsed: 0:09:17.\n",
      " Batch   500 of   835. Elapsed: 0:11:37.\n",
      " Batch   600 of   835. Elapsed: 0:13:56.\n",
      " Batch   700 of   835. Elapsed: 0:16:15.\n",
      " Batch   800 of   835. Elapsed: 0:18:35.\n",
      "\n",
      "  Average training loss: 0.0002356911\n",
      "  Training epcoh took: 0:19:23\n",
      "\n",
      "Runnning Validation...\n",
      "  Accuracy: 0.99342297\n",
      " average val loss = 0.0341544896\n",
      "  Validation took: 0:00:43\n",
      "saving model\n",
      "\n",
      "======== Epoch 29 / 30 ========\n",
      "training...\n",
      " Batch   100 of   835. Elapsed: 0:02:19.\n",
      " Batch   200 of   835. Elapsed: 0:04:38.\n",
      " Batch   300 of   835. Elapsed: 0:06:58.\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs): \n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs)) \n",
    "    print(\"training...\") \n",
    "    t0 = time.time() \n",
    "    # loss initialization \n",
    "    total_loss = 0 \n",
    "    # change model settings to train mode  \n",
    "    model.train()  \n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        # denote time information \n",
    "        if step % 100 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time() - t0) \n",
    "            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed)) \n",
    "        \n",
    "        # put batch inside cpu \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        # extract data from batch \n",
    "        b_input_ids, b_input_mask, b_labels = batch \n",
    "        \n",
    "        # forward propagation \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # compute loss \n",
    "        loss = outputs[0] \n",
    "        \n",
    "        # compute total loss \n",
    "        total_loss += loss.item() \n",
    "        \n",
    "        # backward propagation \n",
    "        loss.backward() \n",
    "        \n",
    "        # gradient clipping \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0) \n",
    "        \n",
    "        # weight parameter update \n",
    "        optimizer.step() \n",
    "        \n",
    "        # reduce learning rate with scheduler \n",
    "        scheduler.step() \n",
    "        \n",
    "        # gradient initialization \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)  \n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.10f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))   \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Runnning Validation...\")\n",
    "    t0 = time.time() \n",
    "    model.eval() # evaluation mode \n",
    "    total_val_loss = 0 \n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        # insert batch inside GPU \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # extract data from batch \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # we are in eval mode and we do not calculate the gradient \n",
    "        with torch.no_grad():     \n",
    "            # Forward \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # calculate loss \n",
    "        logits = outputs[0] \n",
    "\n",
    "        eval_loss += (lossFunc(torch.sigmoid(logits)[:,1], b_labels.float())) \n",
    "\n",
    "        \n",
    "        # move data to CPU \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy() \n",
    "        \n",
    "        # compare logit and labels to derive important metrics \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids) \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)  \n",
    "\n",
    "    print(\"  Accuracy: {0:.8f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\" average val loss = {0:.10f}\".format(avg_val_loss)) \n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0))) \n",
    "    \n",
    "    print(\"saving model\")\n",
    "    torch.save(model, PATH + 'model'  + str(epoch_i))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model with the lowest validation l\n",
    "best_model = torch.load('./storage/bert_multilingual_test/model14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro P6000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,114.    Elapsed: 0:00:46.\n",
      "  Batch   200  of  1,114.    Elapsed: 0:01:33.\n",
      "  Batch   300  of  1,114.    Elapsed: 0:02:20.\n",
      "  Batch   400  of  1,114.    Elapsed: 0:03:06.\n",
      "  Batch   500  of  1,114.    Elapsed: 0:03:53.\n",
      "  Batch   600  of  1,114.    Elapsed: 0:04:39.\n",
      "  Batch   700  of  1,114.    Elapsed: 0:05:26.\n",
      "  Batch   800  of  1,114.    Elapsed: 0:06:12.\n",
      "  Batch   900  of  1,114.    Elapsed: 0:06:59.\n",
      "  Batch 1,000  of  1,114.    Elapsed: 0:07:46.\n",
      "  Batch 1,100  of  1,114.    Elapsed: 0:08:32.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time() \n",
    "predictions = [] \n",
    "for step, batch in enumerate(test_dataloader): \n",
    "    if step % 100 == 0 and not step == 0: \n",
    "        elapsed = format_time(time.time() - t0) \n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch) \n",
    "    b_input_ids, b_input_mask = batch \n",
    "\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        outputs = best_model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask) \n",
    "    \n",
    "    logits = outputs[0] \n",
    "    probs = torch.sigmoid(logits)[:,1] \n",
    "    probs = probs.detach().cpu().numpy() \n",
    "    \n",
    "    predictions.append(probs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1114,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.asarray(predictions) \n",
    "predictions.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = [] \n",
    "for i in range(predictions.shape[0]): \n",
    "    for p in predictions[i]: \n",
    "        final_pred.append(p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.asarray(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142565,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = np.where(final_pred > 0.5, 1, 0).reshape(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./storage/fintech_nlp/sample_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumbission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
